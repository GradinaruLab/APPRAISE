{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMwhSNWEzRCZ"
      },
      "source": [
        "#**ESMFold_advanced batch version**\n",
        "\n",
        "This notebook is a batch implementation version of a ColabFold notebook, and is intended to be run on Google Colaboratory. - Xiaozhe Ding (dingxiaozhe@gmail.com)\n",
        "\n",
        "The original notebook by Sergey Ovchinnikov can be found [here](https://github.com/sokrypton/ColabFold/blob/main/ESMFold.ipynb)\n",
        "\n",
        "for more details regarding ESMFold see: [Github](https://github.com/facebookresearch/esm/tree/main/esm), [Preprint](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)\n",
        "\n",
        "#### **Tips and Instructions**\n",
        "- click the little â–¶ play icon to the left of each cell below.\n",
        "\n",
        "#### **Colab Limitations**\n",
        "- On Tesla T4 (typical free colab GPU), max total length ~ 900\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Mount google drive (run once only)\n",
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-xeUIdrlGOr",
        "outputId": "5aca579b-fef7-46e5-bacc-4065d03411ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "catNptjYwNM6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6024ce-3ddc-4024-9dd8-2188fa4652fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 546 ms, sys: 187 ms, total: 733 ms\n",
            "Wall time: 2min 39s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title ##Installation\n",
        "#@markdown install ESMFold, OpenFold and download Params (~2min 30s)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Installation\n",
        "import os, time\n",
        "if not os.path.isfile(\"esmfold.model\"):\n",
        "  # download esmfold params\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/esmfold.model &\")\n",
        "\n",
        "  # install libs\n",
        "  os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol\")\n",
        "  os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
        "\n",
        "  # install openfold\n",
        "  commit = \"6908936b68ae89f67755240e2f588c09ec31d4c8\"\n",
        "  os.system(f\"pip install -q git+https://github.com/aqlaboratory/openfold.git@{commit}\")\n",
        "\n",
        "  # install esmfold\n",
        "  os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git@beta\")\n",
        "\n",
        "  # wait for Params to finish downloading...\n",
        "  if not os.path.isfile(\"esmfold.model\"):\n",
        "    # backup source!\n",
        "    os.system(\"aria2c -q -x 16 https://files.ipd.uw.edu/pub/esmfold/esmfold.model\")\n",
        "  else:\n",
        "    while os.path.isfile(\"esmfold.model.aria2\"):\n",
        "      time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Load utilitie functions from ColabFold\n",
        "\n",
        "#@markdown utility functions from colabfold\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, TYPE_CHECKING\n",
        "\n",
        "from absl import logging as absl_logging\n",
        "from importlib_metadata import distribution\n",
        "from tqdm import TqdmExperimentalWarning\n",
        "\n",
        "\n",
        "\n",
        "# parse_fasta from colabfold.batch\n",
        "def parse_fasta(fasta_string: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Parses FASTA string and returns list of strings with amino-acid sequences.\n",
        "    Arguments:\n",
        "      fasta_string: The string contents of a FASTA file.\n",
        "    Returns:\n",
        "      A tuple of two lists:\n",
        "      * A list of sequences.\n",
        "      * A list of sequence descriptions taken from the comment lines. In the\n",
        "        same order as the sequences.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    descriptions = []\n",
        "    index = -1\n",
        "    for line in fasta_string.splitlines():\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\">\"):\n",
        "            index += 1\n",
        "            descriptions.append(line[1:])  # Remove the '>' at the beginning.\n",
        "            sequences.append(\"\")\n",
        "            continue\n",
        "        elif not line:\n",
        "            continue  # Skip blank lines.\n",
        "        sequences[index] += line\n",
        "\n",
        "    return sequences, descriptions\n",
        "\n",
        "# get_queries from colabfold.batch\n",
        "def get_queries(\n",
        "    input_path: Union[str, Path], sort_queries_by: str = \"length\"\n",
        ") -> Tuple[List[Tuple[str, str, Optional[List[str]]]], bool]:\n",
        "    \"\"\"Reads a directory of fasta files, a single fasta file or a csv file and returns a tuple\n",
        "    of job name, sequence and the optional a3m lines\"\"\"\n",
        "\n",
        "    input_path = Path(input_path)\n",
        "    if not input_path.exists():\n",
        "        raise OSError(f\"{input_path} could not be found\")\n",
        "\n",
        "    if input_path.is_file():\n",
        "        if input_path.suffix == \".csv\" or input_path.suffix == \".tsv\":\n",
        "            sep = \"\\t\" if input_path.suffix == \".tsv\" else \",\"\n",
        "            df = pandas.read_csv(input_path, sep=sep)\n",
        "            assert \"id\" in df.columns and \"sequence\" in df.columns\n",
        "            queries = [\n",
        "                (seq_id, sequence.upper().split(\":\"), None)\n",
        "                for seq_id, sequence in df[[\"id\", \"sequence\"]].itertuples(index=False)\n",
        "            ]\n",
        "            for i in range(len(queries)):\n",
        "                if len(queries[i][1]) == 1:\n",
        "                    queries[i] = (queries[i][0], queries[i][1][0], None)\n",
        "        elif input_path.suffix == \".a3m\":\n",
        "            (seqs, header) = parse_fasta(input_path.read_text())\n",
        "            if len(seqs) == 0:\n",
        "                raise ValueError(f\"{input_path} is empty\")\n",
        "            query_sequence = seqs[0]\n",
        "            # Use a list so we can easily extend this to multiple msas later\n",
        "            a3m_lines = [input_path.read_text()]\n",
        "            queries = [(input_path.stem, query_sequence, a3m_lines)]\n",
        "        elif input_path.suffix in [\".fasta\", \".faa\", \".fa\"]:\n",
        "            (sequences, headers) = parse_fasta(input_path.read_text())\n",
        "            queries = []\n",
        "            for sequence, header in zip(sequences, headers):\n",
        "                sequence = sequence.upper()\n",
        "                if sequence.count(\":\") == 0:\n",
        "                    # Single sequence\n",
        "                    queries.append((header, sequence, None))\n",
        "                else:\n",
        "                    # Complex mode\n",
        "                    queries.append((header, sequence.upper().split(\":\"), None))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown file format {input_path.suffix}\")\n",
        "    else:\n",
        "        assert input_path.is_dir(), \"Expected either an input file or a input directory\"\n",
        "        queries = []\n",
        "        for file in sorted(input_path.iterdir()):\n",
        "            #troubleshooting\n",
        "            print(\"Parsing fasta file {}\".format(file))\n",
        "\n",
        "            if not file.is_file():\n",
        "                continue\n",
        "            if file.suffix.lower() not in [\".a3m\", \".fasta\", \".faa\"]:\n",
        "                logger.warning(f\"non-fasta/a3m file in input directory: {file}\")\n",
        "                continue\n",
        "            (seqs, header) = parse_fasta(file.read_text())\n",
        "            if len(seqs) == 0:\n",
        "                logger.error(f\"{file} is empty\")\n",
        "                continue\n",
        "            query_sequence = seqs[0]\n",
        "            if len(seqs) > 1 and file.suffix in [\".fasta\", \".faa\", \".fa\"]:\n",
        "                logger.warning(\n",
        "                    f\"More than one sequence in {file}, ignoring all but the first sequence\"\n",
        "                )\n",
        "\n",
        "            if file.suffix.lower() == \".a3m\":\n",
        "                a3m_lines = [file.read_text()]\n",
        "                queries.append((file.stem, query_sequence.upper(), a3m_lines))\n",
        "            else:\n",
        "                if query_sequence.count(\":\") == 0:\n",
        "                    # Single sequence\n",
        "                    queries.append((file.stem, query_sequence, None))\n",
        "                else:\n",
        "                    # Complex mode\n",
        "                    queries.append((file.stem, query_sequence.upper().split(\":\"), None))\n",
        "\n",
        "    # sort by seq. len\n",
        "    if sort_queries_by == \"length\":\n",
        "        queries.sort(key=lambda t: len(t[1]))\n",
        "    elif sort_queries_by == \"random\":\n",
        "        random.shuffle(queries)\n",
        "    is_complex = False\n",
        "    for job_number, (raw_jobname, query_sequence, a3m_lines) in enumerate(queries):\n",
        "        if isinstance(query_sequence, list):\n",
        "            is_complex = True\n",
        "            break\n",
        "        if a3m_lines is not None and a3m_lines[0].startswith(\"#\"):\n",
        "            a3m_line = a3m_lines[0].splitlines()[0]\n",
        "            tab_sep_entries = a3m_line[1:].split(\"\\t\")\n",
        "            if len(tab_sep_entries) == 2:\n",
        "                query_seq_len = tab_sep_entries[0].split(\",\")\n",
        "                query_seq_len = list(map(int, query_seq_len))\n",
        "                query_seqs_cardinality = tab_sep_entries[1].split(\",\")\n",
        "                query_seqs_cardinality = list(map(int, query_seqs_cardinality))\n",
        "                is_single_protein = (\n",
        "                    True\n",
        "                    if len(query_seq_len) == 1 and query_seqs_cardinality[0] == 1\n",
        "                    else False\n",
        "                )\n",
        "                if not is_single_protein:\n",
        "                    is_complex = True\n",
        "                    break\n",
        "    return queries, is_complex\n",
        "\n",
        "# TqdmHandler from colabfold.utils (needed for setup_logging)\n",
        "class TqdmHandler(logging.StreamHandler):\n",
        "    \"\"\"https://stackoverflow.com/a/38895482/3549270\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        logging.StreamHandler.__init__(self)\n",
        "\n",
        "    def emit(self, record):\n",
        "        # We need the native tqdm here\n",
        "        from tqdm import tqdm\n",
        "\n",
        "        msg = self.format(record)\n",
        "        tqdm.write(msg)\n",
        "\n",
        "# setup_logging from colabfold.utils\n",
        "def setup_logging(log_file: Path):\n",
        "    log_file.parent.mkdir(exist_ok=True, parents=True)\n",
        "    root = logging.getLogger()\n",
        "    if root.handlers:\n",
        "        for handler in root.handlers:\n",
        "            root.removeHandler(handler)\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s %(message)s\",\n",
        "        handlers=[TqdmHandler(), logging.FileHandler(log_file)],\n",
        "    )\n",
        "    # otherwise jax will tell us about its search for devices\n",
        "    absl_logging.set_verbosity(\"error\")\n",
        "    warnings.simplefilter(action=\"ignore\", category=TqdmExperimentalWarning)\n",
        "\n",
        "#from colabfold.utils\n",
        "def safe_filename(file: str) -> str:\n",
        "    return \"\".join([c if c.isalnum() or c in [\"_\", \".\", \"-\"] else \"_\" for c in file])\n",
        "\n",
        "# default_data_dir from colabfold.\n",
        "import appdirs\n",
        "default_data_dir = Path(appdirs.user_cache_dir(__package__ or \"colabfold\"))\n"
      ],
      "metadata": {
        "id": "dA9AbDEnFOJU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rqTrvifh17B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ##Load input fastas\n",
        "%%time\n",
        "from string import ascii_uppercase, ascii_lowercase\n",
        "import hashlib, re, os\n",
        "import numpy as np\n",
        "from jax.tree_util import tree_map\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "\n",
        "def parse_output(output):\n",
        "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
        "  plddt = output[\"plddt\"][0,:,1]\n",
        "  \n",
        "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
        "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
        "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
        "  xyz = output[\"positions\"][-1,0,:,1]\n",
        "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
        "  o = {\"pae\":pae[mask,:][:,mask],\n",
        "       \"plddt\":plddt[mask],\n",
        "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
        "       \"xyz\":xyz[mask]}\n",
        "  if \"contacts\" in output[\"lm_output\"]:\n",
        "    lm_contacts = output[\"lm_output\"][\"contacts\"].astype(float)[0]\n",
        "    o[\"lm_contacts\"] = lm_contacts[mask,:][:,mask]\n",
        "  return o\n",
        "\n",
        "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "\n",
        "#################\n",
        "### Load data ###\n",
        "#################\n",
        "\n",
        "#@title Input protein sequence, then hit `Runtime` -> `Run all`\n",
        "\n",
        "#input_dir = '/content/drive/MyDrive/AF2/AAV100_stage_1/stage_1_grouping_2_input_fasta' #@param {type:\"string\"}\n",
        "input_dir = '/content/drive/MyDrive/input_fasta' #@param {type:\"string\"}\n",
        "\n",
        "#result_dir = '/content/drive/MyDrive/AF2/ESMFold_AAV100_stage1_results' #@param {type:\"string\"}\n",
        "result_dir = '/content/drive/MyDrive/AF2/results' #@param {type:\"string\"}\n",
        "\n",
        "#Load queries\n",
        "queries, is_complex = get_queries(input_dir)\n",
        "\n",
        "\n",
        "# from batch.py\n",
        "data_dir = default_data_dir\n",
        "data_dir = Path(data_dir)\n",
        "result_dir = Path(result_dir)\n",
        "result_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# jobname = \"test\" #@param {type:\"string\"}\n",
        "# jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
        "\n",
        "# sequence = \"GWSTELEKHREELKEFLKKEGITNVEIRIDNGRLEVRVEGGTERLKRFLEELRQKLEKKGYTVDIKIE\" #@param {type:\"string\"}\n",
        "# sequence = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
        "# sequence = re.sub(\":+\",\":\",sequence)\n",
        "# sequence = re.sub(\"^[:]+\",\"\",sequence)\n",
        "# sequence = re.sub(\"[:]+$\",\"\",sequence)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ###**Advanced Options**\n",
        "num_recycles = 3 #@param [\"0\", \"1\", \"2\", \"3\", \"6\", \"12\"] {type:\"raw\"}\n",
        "# get_LM_contacts = False #@param {type:\"boolean\"}\n",
        "\n",
        "# copies = 1 #@param {type:\"integer\"}\n",
        "glycine_linker_length = 30 #@param {type:\"number\"}\n",
        "# if copies == \"\" or copies <= 0: copies = 1\n",
        "# sequence = \":\".join([sequence] * copies)\n",
        "\n",
        "#@markdown **sampling options (experimental)**\n",
        "#@markdown - Samples are generated via random masking (defined by `masking_rate`) \n",
        "#@markdown of input sequence (stochastic_mode=\"LM\") and/or via dropout within structure module (stochastic_mode=\"SM\").\n",
        "# samples = None #@param [\"None\", \"1\", \"4\", \"8\", \"16\", \"32\", \"64\"] {type:\"raw\"}\n",
        "# masking_rate = 0.15 #@param {type:\"number\"}\n",
        "# stochastic_mode = \"LM\" #@param [\"LM\", \"LM_SM\", \"SM\"]\n",
        "\n",
        "# ID = jobname+\"_\"+get_hash(sequence)[:5]\n",
        "# seqs = sequence.split(\":\")\n",
        "# lengths = [len(s) for s in seqs]\n",
        "# length = sum(lengths)\n",
        "# print(\"length\",length)\n",
        "\n",
        "# u_seqs = list(set(seqs))\n",
        "# if len(seqs) == 1: mode = \"mono\"\n",
        "# elif len(u_seqs) == 1: mode = \"homo\"\n",
        "# else: mode = \"hetero\"\n",
        "\n",
        "if \"model\" not in dir():\n",
        "  import torch\n",
        "  model = torch.load(\"esmfold.model\")\n",
        "  model.cuda().requires_grad_(False)\n",
        "\n",
        "# # optimized for Tesla T4\n",
        "# if length > 700:\n",
        "#   model.trunk.set_chunk_size(64)\n",
        "# else:\n",
        "#   model.trunk.set_chunk_size(128)\n",
        "\n",
        "best_pdb_str = None\n",
        "best_ptm = 0\n",
        "best_output = None\n",
        "traj = []\n",
        "\n",
        "num_samples = 1 if samples is None else samples\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Run **ESMFold** in batch\n",
        "\n",
        "###########\n",
        "## Batch ##\n",
        "###########\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "keep_existing_results = False\n",
        "\n",
        "if 'logging_setup' not in globals():\n",
        "    setup_logging(Path(result_dir).joinpath(\"log.txt\"))\n",
        "    logging_setup = True\n",
        "\n",
        "\n",
        "for job_number, (raw_jobname, query_sequence, a3m_lines) in enumerate(queries):\n",
        "    jobname = safe_filename(raw_jobname)\n",
        "    # In the colab version and with --zip we know we're done when a zip file has been written\n",
        "    result_zip = result_dir.joinpath(jobname).with_suffix(\".result.zip\")\n",
        "    if keep_existing_results and result_zip.is_file():\n",
        "        logger.info(f\"Skipping {jobname} (result.zip)\")\n",
        "        continue\n",
        "    # In the local version we use a marker file\n",
        "    is_done_marker = result_dir.joinpath(jobname + \".done.txt\")\n",
        "    if keep_existing_results and is_done_marker.is_file():\n",
        "        logger.info(f\"Skipping {jobname} (already done)\")\n",
        "        continue\n",
        "\n",
        "    query_sequence_len = (\n",
        "        len(query_sequence)\n",
        "        if isinstance(query_sequence, str)\n",
        "        else sum(len(s) for s in query_sequence)\n",
        "    )\n",
        "    logger.info(\n",
        "        f\"Query {job_number + 1}/{len(queries)}: {jobname} (length {query_sequence_len})\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # #add glycine linker if there isn't one\n",
        "    glycine_linker_seq = 'G' * glycine_linker_length\n",
        "    sequence = (\n",
        "        query_sequence\n",
        "        if isinstance(query_sequence, str)\n",
        "        else glycine_linker_seq.join(query_sequence)\n",
        "    )\n",
        "\n",
        "    print('> Sequence to model: ')\n",
        "    print(sequence)\n",
        "\n",
        "    if len(sequence) > 700:\n",
        "        model.trunk.set_chunk_size(64)\n",
        "    else:\n",
        "        model.trunk.set_chunk_size(128)\n",
        "\n",
        "    for seed in range(num_samples):\n",
        "        torch.cuda.empty_cache()\n",
        "        if samples is None:\n",
        "            seed = \"default\"\n",
        "            mask_rate = 0.0\n",
        "            model.train(False)\n",
        "        else:\n",
        "            torch.manual_seed(seed)\n",
        "            mask_rate = masking_rate if \"LM\" in stochastic_mode else 0.0\n",
        "            model.train(\"SM\" in stochastic_mode)\n",
        "\n",
        "        output = model.infer(sequence,\n",
        "                            num_recycles=num_recycles, #deleted argument chain_linker = \"X\"*chain_linker, from Alphafold-multimer\n",
        "                            residue_index_offset=512,\n",
        "                            mask_rate=mask_rate,\n",
        "                            return_contacts=get_LM_contacts)\n",
        "        \n",
        "        pdb_str = model.output_to_pdb(output)[0]\n",
        "        output = tree_map(lambda x: x.cpu().numpy(), output)\n",
        "        ptm = output[\"ptm\"][0]\n",
        "        plddt = output[\"plddt\"][0,:,1].mean()\n",
        "        traj.append(parse_output(output))\n",
        "        print(f'{seed} ptm: {ptm:.3f} plddt: {plddt:.1f}')\n",
        "        if ptm > best_ptm:\n",
        "            best_pdb_str = pdb_str\n",
        "            best_ptm = ptm\n",
        "            best_output = output\n",
        "        #os.system(f\"mkdir -p {ID}\")\n",
        "        if samples is None:\n",
        "            pdb_filename = result_dir.joinpath(f\"{jobname}_unrelaxed_ptm{ptm:.3f}_r{num_recycles}_seed{seed}.pdb\")\n",
        "        else:\n",
        "            pdb_filename = result_dir.joinpath(f\"{jobname}_unrelaxed_ptm{ptm:.3f}_r{num_recycles}_seed{seed}_{stochastic_mode}_m{masking_rate:.2f}.pdb\")\n",
        "\n",
        "        with open(pdb_filename,\"w\") as out:\n",
        "            out.write(pdb_str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # try:\n",
        "    #     if a3m_lines is not None:\n",
        "    #         if use_templates is False:\n",
        "    #             (\n",
        "    #                 unpaired_msa,\n",
        "    #                 paired_msa,\n",
        "    #                 query_seqs_unique,\n",
        "    #                 query_seqs_cardinality,\n",
        "    #                 template_features,\n",
        "    #             ) = unserialize_msa(a3m_lines, query_sequence)\n",
        "    #         else:\n",
        "    #             (\n",
        "    #                 unpaired_msa,\n",
        "    #                 paired_msa,\n",
        "    #                 query_seqs_unique,\n",
        "    #                 query_seqs_cardinality,\n",
        "    #             ) = unserialize_msa(a3m_lines, query_sequence)[:4]\n",
        "    #             template_features = get_msa_and_templates(\n",
        "    #                 jobname,\n",
        "    #                 query_sequence,\n",
        "    #                 result_dir,\n",
        "    #                 msa_mode,\n",
        "    #                 use_templates,\n",
        "    #                 custom_template_path,\n",
        "    #                 pair_mode,\n",
        "    #                 host_url,\n",
        "    #             )[4]\n",
        "    #     else:\n",
        "    #         (\n",
        "    #             unpaired_msa,\n",
        "    #             paired_msa,\n",
        "    #             query_seqs_unique,\n",
        "    #             query_seqs_cardinality,\n",
        "    #             template_features,\n",
        "    #         ) = get_msa_and_templates(\n",
        "    #             jobname,\n",
        "    #             query_sequence,\n",
        "    #             result_dir,\n",
        "    #             msa_mode,\n",
        "    #             use_templates,\n",
        "    #             custom_template_path,\n",
        "    #             pair_mode,\n",
        "    #             host_url,\n",
        "    #         )\n",
        "    #     msa = msa_to_str(\n",
        "    #         unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality\n",
        "    #     )\n",
        "    #     result_dir.joinpath(jobname + \".a3m\").write_text(msa)\n",
        "    # except Exception as e:\n",
        "    #     logger.exception(f\"Could not get MSA/templates for {jobname}: {e}\")\n",
        "    #     continue\n",
        "    # try:\n",
        "    #     (input_features, domain_names) = generate_input_feature(\n",
        "    #         query_seqs_unique,\n",
        "    #         query_seqs_cardinality,\n",
        "    #         unpaired_msa,\n",
        "    #         paired_msa,\n",
        "    #         template_features,\n",
        "    #         is_complex,\n",
        "    #         model_type,\n",
        "    #     )\n",
        "    # except Exception as e:\n",
        "    #     logger.exception(f\"Could not generate input features {jobname}: {e}\")\n",
        "    #     continue\n",
        "    # try:\n",
        "    #     query_sequence_len_array = [\n",
        "    #         len(query_seqs_unique[i])\n",
        "    #         for i, cardinality in enumerate(query_seqs_cardinality)\n",
        "    #         for _ in range(0, cardinality)\n",
        "    #     ]\n",
        "\n",
        "    #     # only use padding if we have more than one sequence\n",
        "    #     if sum(query_sequence_len_array) > crop_len:\n",
        "    #         crop_len = math.ceil(sum(query_sequence_len_array) * recompile_padding)\n",
        "\n",
        "    #     outs, model_rank = predict_structure(\n",
        "    #         jobname,\n",
        "    #         result_dir,\n",
        "    #         input_features,\n",
        "    #         is_complex,\n",
        "    #         use_templates,\n",
        "    #         sequences_lengths=query_sequence_len_array,\n",
        "    #         crop_len=crop_len,\n",
        "    #         model_type=model_type,\n",
        "    #         model_runner_and_params=model_runner_and_params,\n",
        "    #         do_relax=use_amber,\n",
        "    #         rank_by=rank_by,\n",
        "    #         stop_at_score=stop_at_score,\n",
        "    #         stop_at_score_below=stop_at_score_below,\n",
        "    #         prediction_callback=prediction_callback,\n",
        "    #         use_gpu_relax=use_gpu_relax,\n",
        "    #         random_seed=random_seed,\n",
        "    #     )\n",
        "    # except RuntimeError as e:\n",
        "    #     # This normally happens on OOM. TODO: Filter for the specific OOM error message\n",
        "    #     logger.error(f\"Could not predict {jobname}. Not Enough GPU memory? {e}\")\n",
        "    #     continue\n",
        "\n",
        "    # Write representations if needed\n",
        "\n",
        "    # representation_files = []\n",
        "\n",
        "    # if save_representations:\n",
        "    #     for i, key in enumerate(model_rank):\n",
        "    #         out = outs[key]\n",
        "    #         model_id = i + 1\n",
        "    #         model_name = out[\"model_name\"]\n",
        "    #         representations = out[\"representations\"]\n",
        "\n",
        "    #         if save_single_representations:\n",
        "    #             single_representation = np.asarray(representations[\"single\"])\n",
        "    #             single_filename = result_dir.joinpath(\n",
        "    #                 f\"{jobname}_single_repr_{model_id}_{model_name}\"\n",
        "    #             )\n",
        "    #             np.save(single_filename, single_representation)\n",
        "\n",
        "    #         if save_pair_representations:\n",
        "    #             pair_representation = np.asarray(representations[\"pair\"])\n",
        "    #             pair_filename = result_dir.joinpath(\n",
        "    #                 f\"{jobname}_pair_repr_{model_id}_{model_name}\"\n",
        "    #             )\n",
        "    #             np.save(pair_filename, pair_representation)\n",
        "\n",
        "    # Write alphafold-db format (PAE)\n",
        "    # alphafold_pae_file = result_dir.joinpath(\n",
        "    #     jobname + \"_predicted_aligned_error_v1.json\"\n",
        "    # )\n",
        "    # alphafold_pae_file.write_text(get_pae_json(outs[0][\"pae\"], outs[0][\"max_pae\"]))\n",
        "    # num_alignment = (\n",
        "    #     int(input_features[\"num_alignments\"])\n",
        "    #     if model_type.startswith(\"AlphaFold2-multimer\")\n",
        "    #     else input_features[\"num_alignments\"][0]\n",
        "    # )\n",
        "    # msa_plot = plot_msa(\n",
        "    #     input_features[\"msa\"][0:num_alignment],\n",
        "    #     input_features[\"msa\"][0],\n",
        "    #     query_sequence_len_array,\n",
        "    #     query_sequence_len,\n",
        "    #     dpi=dpi,\n",
        "    # )\n",
        "    # coverage_png = result_dir.joinpath(jobname + \"_coverage.png\")\n",
        "    # msa_plot.savefig(str(coverage_png))\n",
        "    # msa_plot.close()\n",
        "    # paes_plot = plot_paes(\n",
        "    #     [outs[k][\"pae\"] for k in model_rank], Ls=query_sequence_len_array, dpi=dpi\n",
        "    # )\n",
        "    # pae_png = result_dir.joinpath(jobname + \"_PAE.png\")\n",
        "    # paes_plot.savefig(str(pae_png))\n",
        "    # paes_plot.close()\n",
        "    # plddt_plot = plot_plddts(\n",
        "    #     [outs[k][\"plddt\"] for k in model_rank], Ls=query_sequence_len_array, dpi=dpi\n",
        "    # )\n",
        "    # plddt_png = result_dir.joinpath(jobname + \"_plddt.png\")\n",
        "    # plddt_plot.savefig(str(plddt_png))\n",
        "    # plddt_plot.close()\n",
        "    # result_files = [\n",
        "    #     bibtex_file,\n",
        "    #     config_out_file,\n",
        "    #     alphafold_pae_file,\n",
        "    #     result_dir.joinpath(jobname + \".a3m\"),\n",
        "    #     pae_png,\n",
        "    #     coverage_png,\n",
        "    #     plddt_png,\n",
        "    #     *representation_files,\n",
        "    # ]\n",
        "    # if use_templates:\n",
        "    #     templates_file = result_dir.joinpath(\n",
        "    #         jobname + \"_template_domain_names.json\"\n",
        "    #     )\n",
        "    #     templates_file.write_text(json.dumps(domain_names))\n",
        "    #     result_files.append(templates_file)\n",
        "\n",
        "    # for i, key in enumerate(model_rank):\n",
        "    #     result_files.append(\n",
        "    #         result_dir.joinpath(\n",
        "    #             f\"{jobname}_unrelaxed_rank_{i + 1}_{outs[key]['model_name']}.pdb\"\n",
        "    #         )\n",
        "    #     )\n",
        "    #     result_files.append(\n",
        "    #         result_dir.joinpath(\n",
        "    #             f\"{jobname}_unrelaxed_rank_{i + 1}_{outs[key]['model_name']}_scores.json\"\n",
        "    #         )\n",
        "    #     )\n",
        "    #     if use_amber:\n",
        "    #         result_files.append(\n",
        "    #             result_dir.joinpath(\n",
        "    #                 f\"{jobname}_relaxed_rank_{i + 1}_{outs[key]['model_name']}.pdb\"\n",
        "    #             )\n",
        "    #         )\n",
        "\n",
        "    # if zip_results:\n",
        "    #     with zipfile.ZipFile(result_zip, \"w\") as result_zip:\n",
        "    #         for file in result_files:\n",
        "    #             result_zip.write(file, arcname=file.name)\n",
        "    #     # Delete only after the zip was successful, and also not the bibtex and config because we need those again\n",
        "    #     for file in result_files[2:]:\n",
        "    #         file.unlink()\n",
        "    # else:\n",
        "    #     is_done_marker.touch()\n",
        "    is_done_marker.touch()\n",
        "\n",
        "logger.info(\"Done\")\n"
      ],
      "metadata": {
        "id": "ZHSRzCjXGW4x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Terminate runtime\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "svvaNk22U65M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}